{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADS‑B Flight Segmentation and Visualization\n",
    "This notebook loads ADS‑B logs via SFTP, segments flights by time gaps, calculates metrics, and displays an interactive map with date and\n",
    "irregularity filters to display the anomalous flight paths, and data download option based on the filters selected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports & Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import io\n",
    "import zipfile\n",
    "import json\n",
    "import ast\n",
    "import math\n",
    "import base64\n",
    "import yaml\n",
    "import paramiko\n",
    "import pandas as pd\n",
    "from datetime import datetime, date\n",
    "from IPython.display import display\n",
    "from ipywidgets import DatePicker, FloatSlider, Button, HTML, VBox\n",
    "from ipyleaflet import Map, GeoJSON, Popup, WidgetControl, ControlException\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a YAML configuration file and return its contents as a dictionary.\n",
    "def read_config(path: str) -> dict:\n",
    "    if not os.path.exists(path):\n",
    "        sys.exit(\n",
    "            \"ERROR: ‘config.yml’ not found.\\n\"\n",
    "            \"Please copy `config.example.yml` → `config.yml` and fill in your credentials.\"\n",
    "        )\n",
    "    with open(path) as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "# Load the configuration from the file\n",
    "config = read_config('config.yml')\n",
    "\n",
    "# Extract the base server path from the config and trim any extra whitespace\n",
    "BASE_SERVER_PATH = config['site']['base_server_path'].strip()\n",
    "\n",
    "# Build the full path to the log directory by joining base path and the log path setting\n",
    "LOG_PATH = os.path.join(\n",
    "    BASE_SERVER_PATH,\n",
    "    config['site']['log_path'].strip()\n",
    ")\n",
    "\n",
    "# Print out the resolved log path so you can verify it when the script runs\n",
    "# print(f'Log path: {LOG_PATH}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. SFTP Connection Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_server(cfg: dict) -> paramiko.SSHClient:\n",
    "    \"\"\"\n",
    "    Establish an SSH connection to a remote server using Paramiko.\n",
    "\n",
    "    Parameters:\n",
    "        cfg (dict): Configuration dictionary with a 'site' key containing:\n",
    "            - hostname (str): Server address or IP.\n",
    "            - port (int): SSH port (usually 22).\n",
    "            - username (str): SSH login user.\n",
    "            - password (str, optional): SSH password (if not using key‑based auth).\n",
    "\n",
    "    Returns:\n",
    "        paramiko.SSHClient: An active SSHClient connected to the target host.\n",
    "    \"\"\"\n",
    "    ssh = paramiko.SSHClient()  \n",
    "    ssh.load_system_host_keys()  \n",
    "    # Automatically add unknown host keys (use with care in production)\n",
    "    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "\n",
    "    site = cfg['site']\n",
    "    ssh.connect(\n",
    "        hostname=site['hostname'],\n",
    "        port=site['port'],\n",
    "        username=site['username'],\n",
    "        password=site.get('password')  # None if not provided\n",
    "    )\n",
    "    return ssh\n",
    "\n",
    "def get_sftp_client(cfg: dict) -> tuple:\n",
    "    \"\"\"\n",
    "    Open an SFTP session over an existing SSH connection.\n",
    "\n",
    "    Parameters:\n",
    "        cfg (dict): Same config dict passed to `connect_to_server`.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - paramiko.SSHClient: The underlying SSH client.\n",
    "            - paramiko.SFTPClient: An SFTP client for file operations.\n",
    "    \"\"\"\n",
    "    ssh = connect_to_server(cfg)\n",
    "    sftp = ssh.open_sftp()  # Start SFTP subsystem over SSH\n",
    "    return ssh, sftp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_remote_zip_path(log_date: date, base_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Construct the full path to a remote ZIP log file for a given date.\n",
    "\n",
    "    Parameters:\n",
    "        log_date (date): The date of the log you want, e.g., datetime.date(2022, 10, 4).\n",
    "        base_path (str): The directory on the server where log ZIPs are stored.\n",
    "\n",
    "    Returns:\n",
    "        str: A filepath like \"/remote/logs/adsblog_ny0.txt.2022100400.zip\".\n",
    "    \"\"\"\n",
    "    # Format the date as YYYYMMDD and append \"00.zip\" to match the naming convention\n",
    "    fname = f\"adsblog_ny0.txt.{log_date.strftime('%Y%m%d')}00.zip\"\n",
    "    # Join the base directory and filename into a single, OS‑aware path string\n",
    "    return os.path.join(base_path, fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to remove once we’ve flattened the ADS‑B payload into tabular form\n",
    "COLUMNS_TO_DROP = [\n",
    "    'nav_altitude_fms', 'seen', 'messages', 'tisb', 'mlat', 'gva',\n",
    "    'sil_type', 'sil', 'nac_v', 'nac_p', 'nic_baro', 'version', 'nic',\n",
    "    'rc', 'nav_modes', 'squawk', 'track_rate', 'roll', 'tas', 'ias',\n",
    "    'mach', 'mag_heading', 'geom_rate', 'nav_heading', 'baro_rate',\n",
    "    'nav_altitude_mcp', 'nav_qnh', 'seen_pos', 'sda', 'category'\n",
    "]\n",
    "\n",
    "def load_and_clean_adsb_zip_json(sftp, remote_zip_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Download a zipped ADS‑B log file over SFTP, extract JSON lines,\n",
    "    filter for new ADS‑B messages, and return a cleaned DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        sftp (paramiko.SFTPClient): Open SFTP session on the remote server.\n",
    "        remote_zip_path (str): Full path to the .zip archive on the server.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Table of ADS‑B payload fields + 'date', minus unused columns.\n",
    "    \"\"\"\n",
    "    # 1. Read the raw ZIP bytes from the remote file\n",
    "    with sftp.open(remote_zip_path, 'rb') as rf:\n",
    "        raw = rf.read()\n",
    "\n",
    "    # 2. Load the ZIP archive from bytes\n",
    "    with zipfile.ZipFile(io.BytesIO(raw)) as zf:\n",
    "        # List all entries that are not directories\n",
    "        all_files = [n for n in zf.namelist() if not n.endswith('/')]\n",
    "        # Prefer the .txt file, or fall back to the first member\n",
    "        member = next((n for n in all_files if n.lower().endswith('.txt')), all_files[0])\n",
    "\n",
    "        rows = []\n",
    "        # 3. Stream each line from the chosen member file\n",
    "        with zf.open(member) as f:\n",
    "            for raw_line in f:\n",
    "                try:\n",
    "                    # Decode and parse the JSON object\n",
    "                    obj = json.loads(raw_line.decode('utf-8').strip())\n",
    "                    # Only keep records of type 'new_adsb'\n",
    "                    if obj.get('type') == 'new_adsb':\n",
    "                        p = obj['payload']\n",
    "                        p['date'] = obj['dt']  # Preserve the timestamp\n",
    "                        rows.append(p)\n",
    "                except:\n",
    "                    # Skip any malformed lines silently\n",
    "                    continue\n",
    "\n",
    "    # 4. Build a DataFrame and drop all columns we don't need\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df.drop(columns=COLUMNS_TO_DROP, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_adsb_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert the 'date' column to real datetime objects and\n",
    "    sort the DataFrame by aircraft identifier and timestamp.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Raw ADS‑B DataFrame with a 'date' column.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Cleaned DataFrame with:\n",
    "            - 'date' as datetime dtype (invalid parsing becomes NaT)\n",
    "            - Rows sorted by ['hex', 'date']\n",
    "            - Continuous integer index (ignore_index=True)\n",
    "    \"\"\"\n",
    "    # Ensure 'date' is a pandas datetime, coercing invalid strings to NaT\n",
    "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "    # Sort first by the aircraft 'hex' code, then by timestamp\n",
    "    # Reset the index so it's contiguous after sorting\n",
    "    return df.sort_values(['hex', 'date'], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_time_gap(group: pd.DataFrame, max_gap_minutes: int = 10) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Split a sequence of ADS‑B messages for one flight into segments whenever\n",
    "    there is a gap larger than `max_gap_minutes` between consecutive points.\n",
    "\n",
    "    Parameters:\n",
    "        group (pd.DataFrame): DataFrame for a single flight (e.g., grouped by 'hex' & 'flight'),\n",
    "                              containing at least a 'date' column of dtype datetime64.\n",
    "        max_gap_minutes (int): Threshold (in minutes) above which a new segment starts.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The same rows, with two new columns:\n",
    "            - 'time_diff': gap to the previous point in minutes (NaN for the first row)\n",
    "            - 'segment': integer segment ID (starting at 0), incremented each time gap > threshold\n",
    "    \"\"\"\n",
    "    # 1. Ensure rows are in time order before computing differences\n",
    "    group = group.sort_values('date')\n",
    "\n",
    "    # 2. Compute time difference (in minutes) between each timestamp and its predecessor\n",
    "    group['time_diff'] = group['date'].diff().dt.total_seconds() / 60\n",
    "\n",
    "    # 3. Create a segment counter: start a new segment whenever time_diff > max_gap_minutes\n",
    "    #    .cumsum() will increment the segment ID each time the condition is True (treated as 1)\n",
    "    group['segment'] = (group['time_diff'] > max_gap_minutes).cumsum()\n",
    "\n",
    "    return group\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_adsb(df: pd.DataFrame, max_gap: int = 10) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Divide the full ADS‑B DataFrame into time‑continuous segments for each flight.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Cleaned ADS‑B DataFrame containing at least\n",
    "                           'hex', 'flight', and 'date' columns.\n",
    "        max_gap (int): Maximum gap in minutes before a new segment starts.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Original DataFrame plus 'time_diff' and 'segment' columns,\n",
    "                      with a fresh integer index.\n",
    "    \"\"\"\n",
    "    # 1. Group by aircraft ('hex') and flight ID, without inserting group keys into the index\n",
    "    # 2. Apply split_by_time_gap to each group to compute time differences and segment IDs\n",
    "    # 3. Reset the combined index so rows are numbered 0..n-1\n",
    "    return (\n",
    "        df\n",
    "        .groupby(['hex', 'flight'], group_keys=False)\n",
    "        .apply(lambda grp: split_by_time_gap(grp, max_gap))\n",
    "        .reset_index(drop=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_flight_segments(g: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Summarize a single flight’s segment into start/end times, total duration, and the path.\n",
    "\n",
    "    Parameters:\n",
    "        g (pd.DataFrame): A DataFrame for one flight segment, containing at least:\n",
    "            - 'date' (datetime64): Timestamps of each position report.\n",
    "            - 'lat', 'lon' (float): Latitude and longitude of each report.\n",
    "\n",
    "    Returns:\n",
    "        dict: A summary with keys:\n",
    "            - 'start_time' (Timestamp): Earliest timestamp in this segment.\n",
    "            - 'end_time' (Timestamp): Latest timestamp in this segment.\n",
    "            - 'duration' (float): Total flight time in minutes.\n",
    "            - 'coordinates' (List[tuple]): Ordered list of (lat, lon) pairs.\n",
    "    \"\"\"\n",
    "    # 1. Drop any records missing coordinates, then sort chronologically\n",
    "    pts = g.dropna(subset=['lat', 'lon']).sort_values('date')\n",
    "\n",
    "    # 2. Build a list of (latitude, longitude) tuples in time order\n",
    "    coords = list(zip(pts['lat'], pts['lon']))\n",
    "\n",
    "    # 3. Compute summary metrics\n",
    "    start = g['date'].min()\n",
    "    end   = g['date'].max()\n",
    "    # Duration in minutes between first and last timestamps\n",
    "    duration = (end - start).total_seconds() / 60\n",
    "\n",
    "    return {\n",
    "        'start_time':  start,\n",
    "        'end_time':    end,\n",
    "        'duration':    duration,\n",
    "        'coordinates': coords\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_segments(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Roll up each flight segment into a summary record using `aggregate_flight_segments`.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): ADS‑B DataFrame that has been segmented, containing columns:\n",
    "            - 'hex', 'flight', 'segment' for grouping\n",
    "            - 'date', 'lat', 'lon' (and any other payload fields used by the aggregator)\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: One row per (hex, flight, segment) group with columns:\n",
    "            - 'hex', 'flight', 'segment'\n",
    "            - 'start_time', 'end_time', 'duration', 'coordinates'\n",
    "    \"\"\"\n",
    "    # 1. Group by aircraft code, flight ID, and computed segment\n",
    "    # 2. For each group, call aggregate_flight_segments to get a dict of summary values\n",
    "    # 3. Convert each dict into a pandas Series (so keys become columns)\n",
    "    # 4. Reset the index so that 'hex', 'flight', 'segment' become normal columns again\n",
    "    return (\n",
    "        df\n",
    "        .groupby(['hex', 'flight', 'segment'])\n",
    "        .apply(lambda g: pd.Series(aggregate_flight_segments(g)))\n",
    "        .reset_index()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine(lat1: float, lon1: float, lat2: float, lon2: float) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the great-circle distance between two points on a sphere using the haversine formula.\n",
    "\n",
    "    Parameters:\n",
    "        lat1 (float): Latitude of the first point in decimal degrees.\n",
    "        lon1 (float): Longitude of the first point in decimal degrees.\n",
    "        lat2 (float): Latitude of the second point in decimal degrees.\n",
    "        lon2 (float): Longitude of the second point in decimal degrees.\n",
    "\n",
    "    Returns:\n",
    "        float: Distance between the two points in kilometers.\n",
    "    \"\"\"\n",
    "    # Earth's radius in kilometers (mean radius)\n",
    "    R = 6371.0\n",
    "\n",
    "    # Convert input coordinates from decimal degrees to radians\n",
    "    phi1 = math.radians(lat1)\n",
    "    phi2 = math.radians(lat2)\n",
    "    dphi = math.radians(lat2 - lat1)\n",
    "    dlambda = math.radians(lon2 - lon1)\n",
    "\n",
    "    # Haversine formula components\n",
    "    a = (\n",
    "        math.sin(dphi / 2) ** 2\n",
    "        + math.cos(phi1) * math.cos(phi2) * math.sin(dlambda / 2) ** 2\n",
    "    )\n",
    "    # Central angle between the two points\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "\n",
    "    # Distance on the sphere’s surface\n",
    "    return R * c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_irregularity(coords: list) -> float:\n",
    "    \"\"\"\n",
    "    Measure how “irregular” or meandering a path is by comparing its total length\n",
    "    to the straight‑line distance between start and end points.\n",
    "\n",
    "    Parameters:\n",
    "        coords (List[Tuple[float, float]]): Ordered list of (lat, lon) pairs.\n",
    "            Points with missing coordinates (None) are ignored.\n",
    "\n",
    "    Returns:\n",
    "        float: A value between 0 and 1, where 0 means the path is perfectly straight\n",
    "               (direct distance == total distance) and values closer to 1 indicate\n",
    "               more circuitous routes.\n",
    "    \"\"\"\n",
    "    # 1. Filter out any points that have missing latitude or longitude\n",
    "    v = [pt for pt in coords if None not in pt]\n",
    "    # 2. If there are fewer than two valid points, no “path” exists, so irregularity is 0\n",
    "    if len(v) < 2:\n",
    "        return 0.0\n",
    "\n",
    "    # 3. Compute the sum of distances between each pair of consecutive points\n",
    "    total = sum(\n",
    "        haversine(v[i][0], v[i][1], v[i+1][0], v[i+1][1])\n",
    "        for i in range(len(v) - 1)\n",
    "    )\n",
    "\n",
    "    # 4. Compute the straight‑line (great‑circle) distance from the first to last point\n",
    "    direct = haversine(v[0][0], v[0][1], v[-1][0], v[-1][1])\n",
    "\n",
    "    # 5. If total path length is zero (all points identical), define irregularity as 0\n",
    "    if total == 0.0:\n",
    "        return 0.0\n",
    "\n",
    "    # 6. Irregularity: proportion of “extra” distance beyond the direct line\n",
    "    #    (higher means more deviation from straight line)\n",
    "    return 1.0 - (direct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bearing(lat1: float, lon1: float, lat2: float, lon2: float) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the initial bearing (forward azimuth) from point A to point B \n",
    "    on the Earth’s surface, using the haversine-based great-circle formula.\n",
    "\n",
    "    Parameters:\n",
    "        lat1 (float): Latitude of the start point in decimal degrees.\n",
    "        lon1 (float): Longitude of the start point in decimal degrees.\n",
    "        lat2 (float): Latitude of the end point in decimal degrees.\n",
    "        lon2 (float): Longitude of the end point in decimal degrees.\n",
    "\n",
    "    Returns:\n",
    "        float: Bearing in degrees from north (0° ≤ bearing < 360°).\n",
    "    \"\"\"\n",
    "    # Convert input coordinates from degrees to radians\n",
    "    phi1 = math.radians(lat1)\n",
    "    phi2 = math.radians(lat2)\n",
    "    dlon = math.radians(lon2 - lon1)\n",
    "\n",
    "    # Compute components for the atan2 function\n",
    "    x = math.sin(dlon) * math.cos(phi2)\n",
    "    y = (\n",
    "        math.cos(phi1) * math.sin(phi2)\n",
    "        - math.sin(phi1) * math.cos(phi2) * math.cos(dlon)\n",
    "    )\n",
    "\n",
    "    # atan2 returns the angle relative to the y-axis; convert to degrees and normalize\n",
    "    initial_bearing = math.degrees(math.atan2(x, y))\n",
    "    # Normalize bearing to 0–360° by adding 360 and taking modulo 360\n",
    "    return (initial_bearing + 360) % 360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_total_turning(coords: list) -> float:\n",
    "    \"\"\"\n",
    "    Measure the total angular change (turning) along a path of coordinates.\n",
    "\n",
    "    Parameters:\n",
    "        coords (List[Tuple[float, float]]): Ordered list of (lat, lon) points.\n",
    "            Points containing None are filtered out.\n",
    "\n",
    "    Returns:\n",
    "        float: Sum of absolute bearing changes (in degrees) between consecutive legs,\n",
    "               normalized so that any turn >180° is taken the shorter way around.\n",
    "               Returns 0.0 if fewer than three valid points are provided.\n",
    "    \"\"\"\n",
    "    # 1. Filter out any points with missing latitude or longitude\n",
    "    v = [pt for pt in coords if None not in pt]\n",
    "    # 2. If fewer than 3 points, no meaningful \"turn\" can be computed\n",
    "    if len(v) < 3:\n",
    "        return 0.0\n",
    "\n",
    "    # 3. Compute the bearing for each leg between consecutive points\n",
    "    bears = [\n",
    "        calculate_bearing(v[i][0], v[i][1], v[i+1][0], v[i+1][1])\n",
    "        for i in range(len(v) - 1)\n",
    "    ]\n",
    "\n",
    "    total = 0.0\n",
    "    # 4. For each consecutive pair of bearings, compute the smaller angular difference\n",
    "    for i in range(len(bears) - 1):\n",
    "        diff = abs(bears[i+1] - bears[i])\n",
    "        # If the difference crosses the 0°/360° line, take the shorter path\n",
    "        if diff > 180:\n",
    "            diff = 360 - diff\n",
    "        total += diff\n",
    "\n",
    "    return total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute path‑based flight metrics and append them to the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Aggregated flight segments with a 'coordinates' column\n",
    "                           (list of (lat, lon) tuples) for each segment.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The same DataFrame with two new columns:\n",
    "            - 'irregularity': How much the path deviates from a straight line.\n",
    "            - 'total_turning': Cumulative turning angle along the path.\n",
    "    \"\"\"\n",
    "    # Calculate \"irregularity\" for each list of coordinates\n",
    "    df['irregularity'] = df['coordinates'].apply(compute_irregularity)\n",
    "    # Calculate total turning angle for each list of coordinates\n",
    "    df['total_turning'] = df['coordinates'].apply(compute_total_turning)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Plotting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_geojson(df: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Convert a DataFrame of flight segments into a GeoJSON FeatureCollection.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Must include columns:\n",
    "            - 'coordinates': list of (lat, lon) tuples or a stringrepr.\n",
    "            - 'flight':     flight identifier.\n",
    "            - 'irregularity': float metric for detour index.\n",
    "            - 'popup' (opt.): HTML content for map popups.\n",
    "\n",
    "    Returns:\n",
    "        dict: GeoJSON FeatureCollection with one LineString feature per row.\n",
    "    \"\"\"\n",
    "    features = []\n",
    "\n",
    "    # Iterate over each segment record\n",
    "    for _, row in df.iterrows():\n",
    "        coords = row['coordinates']\n",
    "        # If stored as a string, parse safely into a Python list\n",
    "        if isinstance(coords, str):\n",
    "            coords = ast.literal_eval(coords)\n",
    "\n",
    "        # Convert to GeoJSON [lon, lat] pairs, skipping None values\n",
    "        coords_geo = [\n",
    "            [lon, lat]\n",
    "            for lat, lon in coords\n",
    "            if lat is not None and lon is not None\n",
    "        ]\n",
    "        # Only include segments with at least two points\n",
    "        if len(coords_geo) < 2:\n",
    "            continue\n",
    "\n",
    "        # Build the GeoJSON Feature\n",
    "        features.append({\n",
    "            'type': 'Feature',\n",
    "            'geometry': {\n",
    "                'type': 'LineString',\n",
    "                'coordinates': coords_geo\n",
    "            },\n",
    "            'properties': {\n",
    "                'flight':       row.get('flight', 'N/A'),\n",
    "                'irregularity': float(row['irregularity']),\n",
    "                'popup':        row.get('popup', '')\n",
    "            }\n",
    "        })\n",
    "\n",
    "    # Wrap all features in a FeatureCollection\n",
    "    return {\n",
    "        'type': 'FeatureCollection',\n",
    "        'features': features\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Establish SSH & SFTP connections for data retrieval.\n",
    "ssh_client, sftp = get_sftp_client(read_config('config.yml'))\n",
    "# The SSHClient from Paramiko provides an interface to connect to SSH servers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jt/bxf8n9tx0jx2vwrv56lwq4c80000gp/T/ipykernel_89195/3298152229.py:20: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda grp: split_by_time_gap(grp, max_gap))\n",
      "/var/folders/jt/bxf8n9tx0jx2vwrv56lwq4c80000gp/T/ipykernel_89195/3410488623.py:22: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: pd.Series(aggregate_flight_segments(g)))\n"
     ]
    }
   ],
   "source": [
    "# Widget & Control Definitions\n",
    "\n",
    "\n",
    "# Date picker to choose which log date to fetch\n",
    "date_picker = DatePicker(\n",
    "    description=\"Log Date\",\n",
    "    value=datetime(2022, 10, 4).date()\n",
    ")\n",
    "\n",
    "# Slider to filter segments by minimum irregularity\n",
    "irreg_slider = FloatSlider(\n",
    "    description=\"Irregularity ≥\",\n",
    "    min=0.0, max=1.0, step=0.001,\n",
    "    value=0.0,\n",
    "    continuous_update=False,\n",
    "    layout={\"width\": \"80%\"}\n",
    ")\n",
    "\n",
    "# Button to apply both filters\n",
    "apply_btn = Button(\n",
    "    description=\"Apply Filters\",\n",
    "    button_style=\"primary\"\n",
    ")\n",
    "\n",
    "# Placeholder HTML for download link or messages\n",
    "download_html = HTML(\"<em>No data yet</em>\")\n",
    "\n",
    "# Fullscreen overlay displayed during data loading\n",
    "loading_html = HTML(\"\"\"\n",
    "<div style=\"\n",
    "  position:fixed; top:0; left:0; width:100vw; height:100vh;\n",
    "  background:rgba(255,255,255,0.8);\n",
    "  display:flex; align-items:center; justify-content:center;\n",
    "  z-index:10000;\n",
    "\">\n",
    "  <div style=\"\n",
    "    padding:20px 30px;\n",
    "    background:white; border-radius:8px;\n",
    "    box-shadow:0 2px 6px rgba(0,0,0,0.3);\n",
    "    font-size:18px; font-weight:bold;\n",
    "  \">Loading…</div>\n",
    "</div>\n",
    "\"\"\")\n",
    "\n",
    "# Group widgets vertically and wrap in a map control\n",
    "controls    = VBox([date_picker, irreg_slider, apply_btn, download_html])\n",
    "widget_ctrl = WidgetControl(widget=controls, position=\"topright\")\n",
    "loading_ctrl= WidgetControl(widget=loading_html, position=\"topright\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the map, centered at (0,0) for initial view\n",
    "m = Map(\n",
    "    center=(0, 0),\n",
    "    zoom=2,\n",
    "    layout={'width':'100%', 'height':'700px'}  # Larger display area\n",
    ")\n",
    "\n",
    "# Empty GeoJSON layer; will hold flight segment lines\n",
    "geo_layer = GeoJSON(\n",
    "    data={'type':'FeatureCollection','features':[]},\n",
    "    style={'color':'red', 'weight':3, 'opacity':0.8}\n",
    ")\n",
    "\n",
    "# Add the layer and control panel to the map\n",
    "m.add_layer(geo_layer)\n",
    "m.add_control(widget_ctrl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hover Popup Handlers\n",
    "\n",
    "# Track the current popup so we can remove it on hide\n",
    "_popup = None\n",
    "\n",
    "def _on_segment_hover(feature, **kwargs):\n",
    "    \"\"\"Show flight ID & irregularity when mousing over a segment.\"\"\"\n",
    "    global _popup\n",
    "    # Remove previous popup if any\n",
    "    if _popup:\n",
    "        try: m.remove_layer(_popup)\n",
    "        except: pass\n",
    "\n",
    "    # Determine popup coordinates (event or centroid)\n",
    "    coords = kwargs.get('coordinates')\n",
    "    if coords:\n",
    "        lon, lat = coords\n",
    "    else:\n",
    "        line = feature['geometry']['coordinates']\n",
    "        lon = sum(pt[0] for pt in line) / len(line)\n",
    "        lat = sum(pt[1] for pt in line) / len(line)\n",
    "\n",
    "    # Build HTML content\n",
    "    props = feature['properties']\n",
    "    html = HTML(f\"\"\"\n",
    "      <div style=\"\n",
    "        font-size:12px; line-height:1.2em; padding:4px 6px;\n",
    "        background:rgba(255,255,255,0.9); border-radius:4px;\n",
    "        box-shadow:0 1px 3px rgba(0,0,0,0.2);\n",
    "      \">\n",
    "        <b>Flight:</b> {props['flight']}<br>\n",
    "        <b>Irregularity:</b> {props['irregularity']:.3f}\n",
    "      </div>\n",
    "    \"\"\")\n",
    "\n",
    "    # Create and add the popup to the map\n",
    "    _popup = Popup(\n",
    "        location=(lat, lon),\n",
    "        child=html,\n",
    "        close_button=False,\n",
    "        auto_close=False,\n",
    "        close_on_escape_key=False\n",
    "    )\n",
    "    m.add_layer(_popup)\n",
    "\n",
    "def _on_segment_mouseout(feature, **kwargs):\n",
    "    \"\"\"Remove the popup when the mouse leaves the feature.\"\"\"\n",
    "    global _popup\n",
    "    if _popup:\n",
    "        try: m.remove_layer(_popup)\n",
    "        except: pass\n",
    "        _popup = None\n",
    "\n",
    "# Attach handlers to the GeoJSON layer\n",
    "geo_layer.on_hover(_on_segment_hover)\n",
    "geo_layer.on_mouseout(_on_segment_mouseout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Pipeline Definition\n",
    "# State variables to cache the last‐loaded date and DataFrames\n",
    "_last_date = None\n",
    "_df_cleaned = None\n",
    "_df_metrics = None\n",
    "\n",
    "def _run_pipeline(d):\n",
    "    \"\"\"\n",
    "    Load and process the ADS‑B log for date `d`:\n",
    "    1. Download & unzip JSON → DataFrame\n",
    "    2. Clean timestamps & sort\n",
    "    3. Segment by time gaps\n",
    "    4. Aggregate per segment\n",
    "    5. Compute irregularity & turning metrics\n",
    "    \"\"\"\n",
    "    df_raw = load_and_clean_adsb_zip_json(sftp, build_remote_zip_path(d, LOG_PATH))\n",
    "    df_c   = clean_adsb_data(df_raw)\n",
    "    df_s   = segment_adsb(df_c, max_gap=10)\n",
    "    df_a   = aggregate_segments(df_s)\n",
    "    df_m   = compute_metrics(df_a)\n",
    "    return df_c, df_m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_all_filters(_=None):\n",
    "    \"\"\"\n",
    "    Triggered by the Apply Filters button:\n",
    "    - If the date changed, show loading overlay & reload pipeline\n",
    "    - Filter by irregularity threshold\n",
    "    - Update the GeoJSON layer and recenter the map\n",
    "    - Generate a CSV download link\n",
    "    \"\"\"\n",
    "    global _last_date, _df_cleaned, _df_metrics\n",
    "\n",
    "    now = date_picker.value\n",
    "    new_day = (now != _last_date)\n",
    "\n",
    "    # Show loading overlay on new date\n",
    "    if new_day:\n",
    "        try: m.add_control(loading_ctrl)\n",
    "        except ControlException: pass\n",
    "\n",
    "    try:\n",
    "        if new_day:\n",
    "            _df_cleaned, _df_metrics = _run_pipeline(now)\n",
    "            _last_date = now\n",
    "            # Reset slider to data’s median\n",
    "            mi = _df_metrics[\"irregularity\"]\n",
    "            irreg_slider.min = 0.0\n",
    "            irreg_slider.max = float(mi.max())\n",
    "            irreg_slider.value = float(mi.median())\n",
    "\n",
    "        # Apply threshold filter\n",
    "        thr = irreg_slider.value\n",
    "        df_f = _df_metrics[_df_metrics[\"irregularity\"] >= thr]\n",
    "\n",
    "        # Update map layer & recenter\n",
    "        geo_layer.data = build_geojson(df_f)\n",
    "        m.center = (_df_cleaned[\"lat\"].mean(), _df_cleaned[\"lon\"].mean())\n",
    "        m.zoom = 6\n",
    "\n",
    "        # Create base64‑encoded CSV download link\n",
    "        csv_bytes = df_f.to_csv(index=False).encode()\n",
    "        b64 = base64.b64encode(csv_bytes).decode()\n",
    "        download_html.value = (\n",
    "            f'<a href=\"data:text/csv;base64,{b64}\" '\n",
    "            f'download=\"segments_{now}.csv\">Download CSV</a>'\n",
    "        )\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        # No file for that date: clear layer & show error\n",
    "        geo_layer.data = {'type':'FeatureCollection','features':[]}\n",
    "        download_html.value = f\"<span style='color:red;'>No log for {now}</span>\"\n",
    "\n",
    "    finally:\n",
    "        # Hide loading overlay once done\n",
    "        if new_day:\n",
    "            try: m.remove_control(loading_ctrl)\n",
    "            except ControlException: pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jt/bxf8n9tx0jx2vwrv56lwq4c80000gp/T/ipykernel_89195/3298152229.py:20: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda grp: split_by_time_gap(grp, max_gap))\n",
      "/var/folders/jt/bxf8n9tx0jx2vwrv56lwq4c80000gp/T/ipykernel_89195/3410488623.py:22: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: pd.Series(aggregate_flight_segments(g)))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "487a8f5a0bfa4b3382ac80457025bca6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map(center=[44.709366237447696, -74.8841393211297], controls=(ZoomControl(options=['position', 'zoom_in_text',…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Wire the button click to our callback\n",
    "apply_btn.on_click(apply_all_filters)\n",
    "\n",
    "# Trigger initial draw on notebook load\n",
    "apply_all_filters()\n",
    "\n",
    "# Display the map in the notebook\n",
    "from IPython.display import display\n",
    "display(m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
